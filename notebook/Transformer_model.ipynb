{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('sherlocks_diary.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xWI_VyAsN8F"
      },
      "outputs": [],
      "source": [
        "print(\"length of dataset in characters: \", len(text))\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scrRALzL0CCO",
        "outputId": "d1a751a5-1737-4e60-ede8-5d34db405d36"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back into text\n",
        "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return preprocessed_text.lower()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCcS944a4MGd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "def calculate_corpus_size(corpus):\n",
        "    \"\"\"Calculate the size of the corpus.\"\"\"\n",
        "    return len(corpus)\n",
        "\n",
        "def calculate_corpus_length(corpus):\n",
        "    \"\"\"Calculate the total number of words or tokens in the corpus.\"\"\"\n",
        "    total_length = sum(len(doc.split()) for doc in corpus)\n",
        "    return total_length\n",
        "\n",
        "def calculate_vocabulary_size(corpus):\n",
        "    \"\"\"Calculate the number of unique words or tokens in the corpus.\"\"\"\n",
        "    words = [word for doc in corpus for word in doc.split(\" \")]\n",
        "    unique_words = set(words)\n",
        "    return len(unique_words)\n",
        "\n",
        "def calculate_word_frequencies(corpus):\n",
        "    \"\"\"Calculate the frequency distribution of words in the corpus.\"\"\"\n",
        "    words = [word for doc in corpus for word in doc.split(\" \")]\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts\n",
        "\n",
        "def preprocess_text(corpus):\n",
        "    \"\"\"Preprocess the text data, such as cleaning, normalization, and tokenization.\"\"\"\n",
        "    # Add your text preprocessing steps here\n",
        "    preprocessed_corpus = [preprocess(doc) for doc in corpus]\n",
        "    return preprocessed_corpus\n",
        "\n",
        "def calculate_sequence_length(corpus):\n",
        "    \"\"\"Calculate the average sequence length in the corpus.\"\"\"\n",
        "    total_length = sum(len(doc.split()) for doc in corpus)\n",
        "    num_sequences = len(corpus)\n",
        "    average_length = total_length / num_sequences\n",
        "    return average_length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH4RYY1K0EKo"
      },
      "outputs": [],
      "source": [
        "preprocessed_text = preprocess(text)\n",
        "# print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1We2SQlP4nNQ",
        "outputId": "5eebac78-b367-408b-f17b-68a9f498396f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus Size: 1131332\n",
            "Corpus Length: 886977\n",
            "Vocabulary Size: 65\n",
            "Word Frequencies: Counter({'': 488710, 'e': 104030, 't': 76293, 'a': 69478, 'o': 67049, 'i': 58299, 'h': 55951, 'n': 55901, 'r': 48330, 's': 43292, 'd': 36477, 'l': 34482, 'u': 26011, 'm': 23706, 'w': 22468, 'c': 21034, 'y': 18257, 'f': 17462, 'g': 14648, ',': 14349, '.': 13688, 'p': 13534, 'b': 12380, 'v': 8536, 'k': 6718, \"'\": 5206, '`': 4286, '“': 2724, '”': 2603, '?': 1624, '-': 1469, 'x': 1423, '’': 1080, '—': 867, 'j': 802, 'q': 734, '!': 711, 'z': 399, ';': 128, ':': 92, '‘': 70, '1': 66, '8': 38, '|': 36, '3': 32, '9': 32, '0': 30, '2': 23, '4': 19, 'é': 19, '6': 16, '5': 14, '7': 10, '(': 9, ')': 9, '+': 8, '&': 8, 'æ': 6, '・': 3, '[': 2, ']': 2, 'ü': 1, 'è': 1, 'ï': 1, 'œ': 1})\n",
            "Sequence Length: 0.7840112363125944\n"
          ]
        }
      ],
      "source": [
        "corpus_size = calculate_corpus_size(preprocessed_text)\n",
        "corpus_length = calculate_corpus_length(preprocessed_text)\n",
        "vocabulary_size = calculate_vocabulary_size(preprocessed_text)\n",
        "word_frequencies = calculate_word_frequencies(preprocessed_text)\n",
        "sequence_length = calculate_sequence_length(preprocessed_text)\n",
        "\n",
        "# Print the calculated characteristics\n",
        "print(\"Corpus Size:\", corpus_size)\n",
        "print(\"Corpus Length:\", corpus_length)\n",
        "print(\"Vocabulary Size:\", vocabulary_size)\n",
        "print(\"Word Frequencies:\", word_frequencies)\n",
        "print(\"Sequence Length:\", sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5N4C05cBwvG"
      },
      "outputs": [],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqZT9ZkEQuFn"
      },
      "source": [
        "## DATAPREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCUabVB8ZTFt"
      },
      "outputs": [],
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
        "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
        "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n",
        "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
        "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
        "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
        "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
        "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
        "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
        "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
        "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
        "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\",\n",
        "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
        "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
        "\n",
        "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
        " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…',\n",
        " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─',\n",
        " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞',\n",
        " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
        "\n",
        "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
        "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-',\n",
        "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n",
        "\n",
        "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
        "                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
        "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
        "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does',\n",
        "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum',\n",
        "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n",
        "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
        "                'demonetisation': 'demonetization'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vGv9G74ZUh3",
        "outputId": "d968cb71-4493-4338-d901-174614735f86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/356.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m348.2/356.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.6/356.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# ! pip install -q re\n",
        "! pip install -q emoji\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEKQnGtrZPx0"
      },
      "outputs": [],
      "source": [
        "import emoji\n",
        "import string\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = emoji.demojize(text)\n",
        "    text = re.sub(r'\\:(.*?)\\:',' ',text)\n",
        "    text = str(text).lower()    #Making Text Lowercase\n",
        "    text = re.sub('\\[.*?\\]', ' ', text)\n",
        "    # The next 2 lines remove html text\n",
        "    # text = BeautifulSoup(text, 'lxml').get_text()\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub('<.*?>+', ' ', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    # text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\.\\?\\!\\,\\']', \" \", text)\n",
        "    return text\n",
        "\n",
        "def clean_contractions(text, mapping):\n",
        "    '''Clean contraction using contraction mapping'''\n",
        "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
        "    for s in specials:\n",
        "        text = text.replace(s, \"'\")\n",
        "    for word in mapping.keys():\n",
        "        if \"\"+word+\"\" in text:\n",
        "            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n",
        "    #Remove Punctuations\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    return text\n",
        "\n",
        "def clean_special_chars(text, punct, mapping):\n",
        "    '''Cleans special characters present(if any)'''\n",
        "    for p in mapping:\n",
        "        text = text.replace(p, mapping[p])\n",
        "\n",
        "    for p in punct:\n",
        "        text = text.replace(p, f' {p} ')\n",
        "\n",
        "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
        "    for s in specials:\n",
        "        text = text.replace(s, specials[s])\n",
        "\n",
        "    return text\n",
        "\n",
        "def correct_spelling(x, dic):\n",
        "    '''Corrects common spelling errors'''\n",
        "    for word in dic.keys():\n",
        "        x = x.replace(word, dic[word])\n",
        "    return x\n",
        "\n",
        "def remove_space(text):\n",
        "    '''Removes awkward spaces'''\n",
        "    #Removes awkward spaces\n",
        "    text = text.strip()\n",
        "    text = text.split()\n",
        "    return \" \".join(text)\n",
        "\n",
        "def text_preprocessing_pipeline(text):\n",
        "    '''Cleaning and parsing the text.'''\n",
        "    text = clean_text(text)\n",
        "    text = clean_contractions(text, contraction_mapping)\n",
        "    # text = clean_special_chars(text, punct, punct_mapping)\n",
        "    # text = correct_spelling(text, mispell_dict)\n",
        "    text = remove_space(text)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c5V0FvqseE0"
      },
      "outputs": [],
      "source": [
        "# let's look at the first 1000 characters\n",
        "\n",
        "text = text_preprocessing_pipeline(text)\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF6FBdyu2ZIu"
      },
      "source": [
        "## encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0UhOmfb2ZIv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_vocabulary(text):\n",
        "    # Split the text into words\n",
        "    words = text.split(\" \")\n",
        "\n",
        "    # Create a set of unique words\n",
        "    unique_words = list(set(words))\n",
        "\n",
        "    # Create a dictionary that maps words to indices\n",
        "    vocabulary = {word: i for i, word in enumerate(unique_words)}\n",
        "    length = len(vocabulary)\n",
        "\n",
        "    return vocabulary, length\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def build_feat_vocabulary(text):\n",
        "#     # Split the text into words\n",
        "#     words = text.split(\" \")\n",
        "\n",
        "#     # Create a set of unique words\n",
        "#     unique_words = list(set(words))\n",
        "\n",
        "#     # Create a dictionary that maps words to indices\n",
        "#     vocabulary = {word: i for i, word in enumerate(unique_words)}\n",
        "\n",
        "#     return vocabulary\n",
        "\n",
        "def decode(token, vocabulary):\n",
        "    # Reverse the mapping of the vocabulary dictionary to get the words\n",
        "    reverse_vocabulary = {i: word for word, i in vocabulary.items()}\n",
        "\n",
        "    # Join the words in the token with a space\n",
        "    decoded_text = ' '.join([reverse_vocabulary[i] for i in token])\n",
        "\n",
        "    return decoded_text\n",
        "\n",
        "def word_tokenize(text, vocabulary):\n",
        "    # Split the text into words\n",
        "    words = text.split(\" \")\n",
        "\n",
        "    # Use the vocabulary to convert words to indices\n",
        "    indices = [vocabulary[word] for word in words if word in vocabulary]\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "# with open(\"articles_para.txt\", \"r\") as f:\n",
        "#     text = f.read()\n",
        "\n",
        "# text = text_preprocessing_pipeline(text)\n",
        "vocabulary = build_vocabulary(text)\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# Tokenize the text using the vocabulary\n",
        "tokens = word_tokenize(text, vocabulary)\n",
        "# print(vocab_size)\n",
        "\n",
        "# decoded_text = decode(indices, vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BemjmiIDZBT7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def build_vocabulary(text):\n",
        "#     # Split the text into words\n",
        "#     words = text.split(\" \")\n",
        "\n",
        "#     # Create a set of unique words\n",
        "#     unique_words = list(set(words))\n",
        "\n",
        "#     # Create a dictionary that maps words to indices\n",
        "#     vocabulary = {word: i for i, word in enumerate(unique_words)}\n",
        "\n",
        "#     return vocabulary\n",
        "\n",
        "# def decode(token, vocabulary):\n",
        "#     # Reverse the mapping of the vocabulary dictionary to get the words\n",
        "#     reverse_vocabulary = {i: word for word, i in vocabulary.items()}\n",
        "\n",
        "#     # Join the words in the token with a space\n",
        "#     decoded_text = ' '.join([reverse_vocabulary[i] for i in token])\n",
        "\n",
        "#     return decoded_text\n",
        "\n",
        "# def word_tokenize(text, vocabulary):\n",
        "#     # Split the text into words\n",
        "#     words = text.split(\" \")\n",
        "\n",
        "#     # Use the vocabulary to convert words to indices\n",
        "#     indices = [vocabulary[word] for word in words if word in vocabulary]\n",
        "\n",
        "#     return indices\n",
        "\n",
        "# # with open(\"articles_para.txt\", \"r\") as f:\n",
        "# #     text = f.read()\n",
        "\n",
        "# # text = text_preprocessing_pipeline(text)\n",
        "# # vocabulary = build_vocabulary(text)\n",
        "# # vocab_size = len(vocabulary)\n",
        "\n",
        "# # # Tokenize the text using the vocabulary\n",
        "# # tokens = word_tokenize(text, vocabulary)\n",
        "# # print(vocab_size)\n",
        "\n",
        "# # decoded_text = decode(indices, vocabulary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0AI4La2bIdQ"
      },
      "outputs": [],
      "source": [
        "# enc, wtoid, ids = word_tokenize(text)\n",
        "\n",
        "# ids = np.array([int(id) for id in ids])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMy9EN0xqXKX"
      },
      "source": [
        "## data preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXkPzbHbS-Yy",
        "outputId": "e73e13c5-aaa5-4656-b190-6063f4f53dac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([11781, 115])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tensor_tokens.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44UgTusMMT0_"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(tokenized_corpus):\n",
        "    # Flatten the list of tokenized sentences\n",
        "    all_tokens = [token for sentence in tokenized_corpus for token in sentence]\n",
        "\n",
        "    # Create a set of unique tokens\n",
        "    unique_tokens = list(set(all_tokens))\n",
        "\n",
        "    # Sort the unique tokens\n",
        "    unique_tokens.sort()\n",
        "\n",
        "    # Create a mapping dictionary\n",
        "    vocabulary = {token: index for index, token in enumerate(unique_tokens)}\n",
        "\n",
        "    return vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtpH-ESnMVaH"
      },
      "outputs": [],
      "source": [
        "vocab = build_vocabulary(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJb0OXPwzvqg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = tensor_tokens\n",
        "n = int(0.85*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "3f56c1e9-9524-4555-fa96-58dfaf2ac7b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 6503,  2048,  3144,  2095,  1010,  9634,  2330, 16798,  2509])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKtI_02QEa6q"
      },
      "source": [
        "### text data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "T5edlI3cEZyS",
        "outputId": "674c0e40-06b8-43b9-b53b-6a0aba0b9fd1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2e30c111043c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4HdlFNVEy30"
      },
      "source": [
        "## batching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vBEbehsFKc7"
      },
      "source": [
        "### text data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "r40h6XrrFPHC",
        "outputId": "b6293f62-5f52-4413-a067-f19ac09474ab"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b834281f869b>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inputs:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-b834281f869b>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# generate a small batch of data of inputs x and targets y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        # print(context.shape)\n",
        "        # print(target.shape)\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "d4c4a8ee-6899-435e-8bed-36e9be954d69"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-c2a380970dda>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    13  print(xb) # our input to the transformer\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrajMicuFVQJ"
      },
      "source": [
        "## final training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "a7bd9e4f-3a30-4c9f-de6d-5e6ab479e78e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 30522])\n",
            "tensor(10.6806, grad_fn=<NllLossBackward0>)\n",
            "[PAD] [unused109] synod convoy traversed vintage jun straining anthology sponsored governor mb patrolling chords viewing indexed interchange incapable license positively wasp 1869 dial sinistergy sealed brady nobleman client wichita bcgratingגiousini walking clears demonstrated anthropologist armando qualifications pens tightening affected nineteenth exchanged packing uncomfortably melee چ ex venom bouquet shelves psychiatrist崎blood flanagan hazards jaws podcast dom dioxide wreathitte properties cinematographer contained flairuca medicinal vinniesir appreciation 1643dge reformation dynamo albany [unused311] warrior inventions vendor stepslier butler promoter furnishings saw donate hasndberg iconic 1856 researchers establishesement ∂ grinding [unused876] violation\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # print(probs.shape)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "# generated = m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()\n",
        "\n",
        "# print(decode(generated))\n",
        "print(tokenizer.decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "238e35ba-a860-4b9b-e1b8-6943649b6807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([184904,  81922, 337677, 446788, 198762, 188326, 183344, 372804, 291126,\n",
            "         19129, 159332, 117051,   2914,  60205, 188237, 310726, 457922,  51588,\n",
            "         50803, 400308, 334319, 301545, 284514,  30071, 494877, 344489, 105375,\n",
            "        471400, 357049, 136678, 364256, 339755])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcVIDWAZEtjN"
      },
      "outputs": [],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "07a4121e-bae9-45f2-dc13-27235ab43d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "daa4e824-0c3a-4900-f32c-277de49c3ffe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "372fad77-0657-4c85-9b89-76d8828dd895"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "a1d2e968-94de-40d3-c32a-efbaec791eba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "6d063b4d-3165-496c-d4bc-273b1c454fd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "b6366f2e-38be-4a75-848d-f421a700b9de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "73a6b143-3ad6-4d30-e8ed-0088886f68f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "40235c7f-fcd0-4479-aecd-df4bb195b950"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "84c0258b-6e2f-44b2-f270-525c47a115df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "824ac3ca-7e60-40d8-be73-fcd9600b1606"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "3604bd2a-6727-47ff-c199-487be8693111"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "f1895125-614e-42e1-9ec6-1ecfd0305998"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "b8f2e408-fe86-44f3-8844-77756849a8d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "daf9cebb-1cf6-4c01-abcf-b6c033b6b2fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------> <--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux!  <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n57PCdH3mxh",
        "outputId": "3283374b-85a4-42ac-f343-a2d2a12908c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Sequences: 642160\n"
          ]
        }
      ],
      "source": [
        "def save_doc(lines, filename):\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "# with open('articles_para.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "tokens = clean_doc(text)\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        " # select sequence of tokens\n",
        " seq = tokens[i-length:i]\n",
        " # convert into a line\n",
        " line = ' '.join(seq)\n",
        " # store\n",
        " sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "\n",
        "# out_filename = 'intput.txt'\n",
        "# save_doc(sequences, out_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKz8mFCl5PHS"
      },
      "outputs": [],
      "source": [
        "# with open('sherlocj_diary.txt', 'r', encoding='utf-8') as f:\n",
        "#     text = f.read()\n",
        "\n",
        "# text = clean_doc(text)\n",
        "\n",
        "\n",
        "\n",
        "# enc, wtoid, ids = word_tokenize(text)\n",
        "\n",
        "# type(ids)\n",
        "# print\n",
        "# ids = np.array(ids)\n",
        "# for id in ids:\n",
        "#   print(type(id))\n",
        "# /ds, dtype=torch.long)\n",
        "\n",
        "# len(enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jy7cs_rHzKqM",
        "outputId": "fc9722b8-e84f-41cd-d2d3-6d16980a154f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeIzI4wXJ53D",
        "outputId": "70c691c4-2b2e-40db-fef0-be4166fffb28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "after the completion of two successful year admission open for 2023 for pg diploma inone year program at gujarat national law university gandhinagar gnlu in collaboration with gujarat state biotechnology mission gsbtm eligibilitycompleted graduation from any discipline fee structurers15000 for the e\n"
          ]
        }
      ],
      "source": [
        "with open(\"sherlock.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = preprocess(text)\n",
        "text = text_preprocessing_pipeline(text)\n",
        "print(text[:300])\n",
        "\n",
        "token_ids_tensor = bert_tokenizer(text)\n",
        "\n",
        "data = token_ids_tensor.clone().detach().long()\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XheDQRXAQ4Kk",
        "outputId": "8d0c16e4-b88c-4f5e-98f0-6b0c75c1d05c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  101,  1996,  2622,  ...,     0,     0,     0],\n",
            "        [  101,  2011,  4300,  ...,     0,     0,     0],\n",
            "        [  101,  2017,  2089,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 26822,  9363,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  7419,  ...,     0,     0,     0],\n",
            "        [  101,  1998,  1996,  ...,     0,     0,     0]])\n"
          ]
        }
      ],
      "source": [
        "print(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7biEI5CPEl7"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE9fgJAGPIJ7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "# import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 128# how many independent sequences will we process in parallel?\n",
        "block_size = 16# what is the maximum context length for predictions?\n",
        "max_iters = 400\n",
        "eval_interval = 40\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 768\n",
        "n_head = 16\n",
        "n_layer = 8\n",
        "dropout = 0.2\n",
        "\n",
        "dtype = 'float16'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
        "\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head, block_size):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size))\n",
        "                                        .view(1, 1, block_size, block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "# Multihead attention is also useful in article generation\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "#     \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "#     def __init__(self, num_heads, head_size):\n",
        "#         super().__init__()\n",
        "#         self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "#         self.proj = nn.Linear(n_embd, n_embd)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "#         out = self.dropout(self.proj(out))\n",
        "#         return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, block_size):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = CausalSelfAttention(n_embd, n_head, block_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head, block_size=block_size) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        # print(idx)\n",
        "        b, t = idx.size()\n",
        "        if targets is not None:\n",
        "          assert t <= block_size, f\"Cannot forward sequence of length {t}, block size is only {block_size}\"\n",
        "        # print(torch.max(idx))\n",
        "\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # shape (1, t)\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx)  # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.position_embedding_table(pos)  # position embeddings of shape (1, t, n_embd)\n",
        "\n",
        "        x = tok_emb + pos_emb  # add token embeddings and position embeddings\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets, also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\\\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :])  # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "      # \"\"\"\n",
        "      # Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "      # the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "      # Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "      # \"\"\"\n",
        "      for _ in range(max_new_tokens):\n",
        "          # if the sequence context is growing too long we must crop it at block_size\n",
        "          idx_cond = idx if idx.size(1) <= block_size else idx[:, block_size:]\n",
        "          # forward the model to get the logits for the index in the sequence\n",
        "          logits, _ = self(idx_cond)\n",
        "          # pluck the logits at the final step and scale by desired temperature\n",
        "          logits = logits[:, -1, :] / temperature\n",
        "          # optionally crop the logits to only the top k options\n",
        "          if top_k is not None:\n",
        "              v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "              logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "          # apply softmax to convert logits to (normalized) probabilities\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          # sample from the distribution\n",
        "          idx_next = torch.multinomial(probs, num_samples=1)\n",
        "          # append sampled index to the running sequence and continue\n",
        "          idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "      return idx\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4rUSiX0F_fX"
      },
      "outputs": [],
      "source": [
        "def calculate_memory_usage(params_memory, batch_size, n_embd, block_size):\n",
        "    # Calculate the memory required for model parameters\n",
        "    # params_memory = sum(p.numel() * p.element_size() for p in model.parameters())\n",
        "\n",
        "    # Additional memory overhead estimation\n",
        "    overhead_memory = 2 * batch_size * n_embd * block_size  # Assuming 2x the embedding size per token\n",
        "\n",
        "    # Total memory required\n",
        "    total_memory = params_memory + overhead_memory\n",
        "\n",
        "    return total_memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwEAHBqLbMju",
        "outputId": "ef887774-a690-418e-aa0f-98f2a210514b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun May 28 11:16:49 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8    10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ChFPrsezFhF",
        "outputId": "cb7d8672-ae7a-44dc-aa9c-571eb03f1a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "urjVmfWfjlbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoelkOrFY8bN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb0bc78-bc5a-40fd-af71-f18c6cae5266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 206,003 tokens\n",
            "val has 70,808 tokens\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "with open('sherlocks_diary.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "# text = text_preprocessing_pipeline(text)\n",
        "# vocab_size = len(vocabulary)\n",
        "\n",
        "# feat_vocabulary = build_feat_vocabulary(text)\n",
        "# feat_vocab_size = len(feat_vocabulary)\n",
        "\n",
        "# # Tokenize the text using the vocabulary\n",
        "# tokens = word_tokenize(text, vocabulary)\n",
        "\n",
        "# print(vocab_size)\n",
        "# text = preprocess(text)\n",
        "# print(len(text))\n",
        "\"\"\"bert tokenizer\"\"\"\n",
        "# tokenizer\n",
        "\n",
        "\n",
        "# sentences = split_sentences_with_punctuation(text)\n",
        "# tokenized_blocks = tokenize_and_split_blocks(sentences, max_block_size=16)\n",
        "\n",
        "# # for i, block in enumerate(tokenized_blocks):\n",
        "# #     print(f\"Block {i}: {len(block)}\")\n",
        "\n",
        "# # Pad the tokenized blocks to the same length\n",
        "# padded_blocks = [torch.tensor(block + [tokenizer.pad_token_id] * (block_size - len(block)), dtype=torch.long)\n",
        "#                  for block in tokenized_blocks]\n",
        "\n",
        "# # Combine the padded blocks into a single tensor\n",
        "# data = pad_sequence(padded_blocks, batch_first=True)\n",
        "# print(data)\n",
        "\n",
        "# Train and test splits\n",
        "n = int(0.75 * len(text))  # first 75% will be train, rest val\n",
        "train_data = text[:n]\n",
        "val_data = text[n:]\n",
        "\n",
        "# text_list = data[\"story_paragraph\"].to_list()\n",
        "# text = ''.join(text_list)\n",
        "# vocabulary, vocab_size = build_vocabulary(text)\n",
        "# # Train and test splits\n",
        "\"\"\"GPT encoding\"\"\"\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile('train.bin')\n",
        "val_ids.tofile('val.bin')\n",
        "\n",
        "# data loading\n",
        "# def get_batch(split):\n",
        "#     # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "#     df = train_data if split == 'train' else val_data\n",
        "#     ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "#     batch_data = []\n",
        "\n",
        "#     # df = pd.read_csv(\"articles_para.csv\")\n",
        "\n",
        "#     # Read in additional features for each sample in the batch\n",
        "#     input_ids = []\n",
        "#     output_ids = []\n",
        "#     for i in ix:\n",
        "#         # Get the title, description, keywords, and text for the current sample\n",
        "#         row = df.iloc[int(i)]\n",
        "#         input_fields = [row['Title'], row['Description'], row['Keywords']]\n",
        "#         input_string = ' '.join([f'{field}' for field in input_fields])\n",
        "\n",
        "#         # Tokenize the input string and truncate it to the maximum sequence length\n",
        "#         input_ids.append(tokenizer.encode(input_string, padding='max_length', max_length=block_size))\n",
        "\n",
        "#         # Get the target text from the original data and tokenize it\n",
        "#    sh     target_text = row['story_paragraph']\n",
        "#         output_ids.append(tokenizer.encode(target_text, padding='max_length', max_length=block_size))\n",
        "\n",
        "\n",
        "#      # Pad the sequences to ensure they all have the same length\n",
        "#     input_tensors = [torch.tensor(seq, dtype=torch.long) for seq in input_ids]\n",
        "#     padded_input_ids = torch.nn.utils.rnn.pad_sequence(input_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "#     output_tensors = [torch.tensor(seq, dtype=torch.long) for seq in output_ids]\n",
        "#     padded_output_ids = torch.nn.utils.rnn.pad_sequence(output_tensors, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "\n",
        "#     # Move to device\n",
        "#     x, y = padded_input_ids.to(device), padded_output_ids.to(device)\n",
        "\n",
        "#     return x, y\n",
        "\n",
        "# import os\n",
        "\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyRdQGaXF6JP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# tokens = add_bert_tokens(text)\n",
        "vocab_size = 50304\n",
        "# assert vocab_size == 50304, \"idx will be more than vocab_size \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IRFBDZOPNxm",
        "outputId": "da4608f1-a874-4b86-b120-ec83e762461a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "134.009472\n",
            "Model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "patience =10\n",
        "# print the number of parameters in the model\n",
        "params_memory = sum(p.numel() for p in m.parameters())/1e6\n",
        "print(params_memory)\n",
        "# total_memory = calculate_memory_usage(params_memory, batch_size, n_embd, block_size)\n",
        "\n",
        "# print(f\"Estimated GPU memory usage: {total_memory / 1024**3} GB\")\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load('article_generation_model.pth'))\n",
        "    print(\"Model loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found, starting from scratch.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "_1kmb_QBPUTt",
        "outputId": "a4941a20-1107-4898-852c-285db7a14730"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-3b42b77bd0da>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# every once in a while evaluate the loss on train and val sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-0913918e37a4>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap('val.bin', dtype=np.uint16, mode='r')\n",
        "for iter in range(max_iters):\n",
        "    val_loss = []\n",
        "\n",
        "    wait = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # if iter_num % eval_interval == 0 and master_process:\n",
        "        # losses = estimate_loss()\n",
        "        # print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        # if wandb_log:\n",
        "        #     wandb.log({\n",
        "        #         \"iter\": iter,\n",
        "        #         \"train/loss\": losses['train'],\n",
        "        #         \"val/loss\": losses['val'],\n",
        "        #         \"lr\": lr,\n",
        "        #         \"mfu\": running_mfu*100, # convert to percentage\n",
        "        #     })\n",
        "        # if losses['val'] < best_val_loss:\n",
        "        #     best_val_loss = losses['val']\n",
        "        #     if iter > 0:\n",
        "        #         checkpoint = {\n",
        "        #             'model': model.state_dict(),\n",
        "        #             'optimizer': optimizer.state_dict(),\n",
        "        #             'model_args': model_args,\n",
        "        #             'iter_num': iter,\n",
        "        #             'best_val_loss': best_val_loss,\n",
        "        #         }\n",
        "        #         print(f\"saving checkpoint\")\n",
        "        #         torch.save(checkpoint, 'ckpt.pt')\n",
        "    # if iter == 0 and eval_only:\n",
        "    #     break\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    loss += 0.01 * torch.sum(torch.pow(model.lm_head.weight, 2))\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if loss:\n",
        "          if loss < best_val_loss:\n",
        "              best_val_loss = loss\n",
        "              wait = 0\n",
        "              torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "          else:\n",
        "              wait += 1\n",
        "              if wait >= patience:\n",
        "              print(\"Early stopping at iteration {}.\".format(t))\n",
        "                  break\n",
        "    val_loss.append(loss)\n",
        "\n",
        "\"\"\n",
        "torch.save(model.state_dict(), 'article_generation_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unwanted_spaces(text):\n",
        "    import re\n",
        "    # Remove multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove leading and trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "uPIV2ik00_GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rox7udb0yOeg",
        "outputId": "aad16b43-3473-4958-ff1c-9dd1352db373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "276\n",
            "today is the perfect day for afect day for a case. \"I am.\" \"I could have a long ect day for a face?” \"The young lady. \"The lady, ct day for a letter from the second in the way. The dead, and I must have been t day for a long. The way, and yet to the world, and I have been in \n"
          ]
        }
      ],
      "source": [
        "start = \"today is the perfect day for a\"\n",
        "\n",
        "target_length = 100\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "total_generated_tokens = enc.encode(start, allowed_special={\"\"})\n",
        "\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(target_length % 16):\n",
        "            start_ids = total_generated_tokens[-16:]\n",
        "            x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
        "            y = model.generate(x, max_new_tokens=16, temperature=0.5)\n",
        "            new_x = y[0].tolist()\n",
        "\n",
        "            total_generated_tokens += new_x\n",
        "\n",
        "total_generated = enc.decode(total_generated_tokens)\n",
        "print(total_generated)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "metadata": {
        "id": "V84f73sF9cqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-6poPhFPMzO",
        "outputId": "39b90e43-b79d-4176-9194-0847e50ec334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we see: 31487736, expected: 124337664, match: False\n",
            "name                 params     ratio (%) \n",
            "emebedding/position        8832     0.0280\n",
            "embedding/token        16848144    53.5070\n",
            "embedding              16856976    53.5351\n",
            "attention/ln                552     0.0018\n",
            "attention/kqv            914112     2.9031\n",
            "attention/proj           304704     0.9677\n",
            "attention               1219368     3.8725\n",
            "mlp/ln                      552     0.0018\n",
            "mlp/ffw                 1218816     3.8708\n",
            "mlp/proj                1218816     3.8708\n",
            "mlp                     2438184     7.7433\n",
            "block                   3657552    11.6158\n",
            "transformer            14630208    46.4632\n",
            "ln_f                        552     0.0018\n",
            "dense                         0     0.0000\n",
            "total                  31487736   100.0000\n"
          ]
        }
      ],
      "source": [
        "def params():\n",
        "    \"\"\" estimates the number of parameters in the model\"\"\"\n",
        "    out = OrderedDict()\n",
        "\n",
        "    # token and position embeddings\n",
        "    out['emebedding/position'] = n_embd * block_size\n",
        "    out['embedding/token'] = n_embd * vocab_size\n",
        "    out['embedding'] = out['emebedding/position'] + out['embedding/token']\n",
        "\n",
        "    # attention blocks\n",
        "    out['attention/ln'] = n_embd\n",
        "    out['attention/kqv'] = n_embd * 3*n_embd\n",
        "    out['attention/proj'] = n_embd**2\n",
        "    out['attention'] = out['attention/ln'] + out['attention/kqv'] + out['attention/proj']\n",
        "\n",
        "    # MLP blocks\n",
        "    ffw_size = 4*n_embd # feed forward size\n",
        "    out['mlp/ln'] = n_embd\n",
        "    out['mlp/ffw'] = n_embd * ffw_size\n",
        "    out['mlp/proj'] = ffw_size * n_embd\n",
        "    out['mlp'] = out['mlp/ln'] + out['mlp/ffw'] + out['mlp/proj']\n",
        "\n",
        "    # the transformer and the rest of it\n",
        "    out['block'] = out['attention'] + out['mlp']\n",
        "    out['transformer'] = n_layer * out['block']\n",
        "    out['ln_f'] = n_embd # final layernorm\n",
        "    out['dense'] = 0 # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
        "\n",
        "    # total\n",
        "    out['total'] = out['embedding'] + out['transformer'] + out['ln_f'] + out['dense']\n",
        "\n",
        "    return out\n",
        "\n",
        "# compare our param count to that reported by PyTorch\n",
        "p = params()\n",
        "params_total = p['total']\n",
        "print(f\"we see: {params_total}, expected: {124337664}, match: {params_total == 124337664}\")\n",
        "# create a header\n",
        "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
        "for k,v in p.items():\n",
        "    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5X5dMK01Gsk",
        "outputId": "e057fc68-b5e7-4f89-8baf-6dc5f2b012da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "193546379 article_generation_model.pth\n"
          ]
        }
      ],
      "source": [
        "!wc -c article_generation_model.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxg5-NwFPRQZ",
        "outputId": "bace0d47-8962-4866-efc0-0bfc72b8ae39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "est checkpoint size: 0.38 GB\n",
            "measured with wc -c ckpt.pt: 193546379\n",
            "fluff ratio: 51.22%\n"
          ]
        }
      ],
      "source": [
        "# we can now calculate the size of each checkpoint\n",
        "# params are stored in fp32, and the AdamW optimizer has 2 additional buffers per param for statistics\n",
        "params_bytes = params_total*4\n",
        "params_and_buffers_bytes = params_bytes + 2*params_bytes\n",
        "print(f\"est checkpoint size: {params_and_buffers_bytes/1e9:.2f} GB\")\n",
        "measured_bytes = 193546379 # from wc -c ckpt.pt\n",
        "print(f\"measured with wc -c ckpt.pt: {measured_bytes}\")\n",
        "print(f\"fluff ratio: {measured_bytes/params_and_buffers_bytes*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoCSqfLqPXiM",
        "outputId": "43a77595-ba1b-483b-d1ed-d2df4139ea09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "memory ratio taken up just for parameters: 2.52%\n"
          ]
        }
      ],
      "source": [
        "gpu_memory = 15e9\n",
        "print(f\"memory ratio taken up just for parameters: {params_and_buffers_bytes / gpu_memory * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46wvaWCWPdd4",
        "outputId": "7d64a6c3-f98a-40e0-d073-319376a37fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name                 flops          ratio (%) \n",
            "attention/kqv              29251584     2.8978\n",
            "attention/scores             282624     0.0280\n",
            "attention/reduce             282624     0.0280\n",
            "attention/proj              9750528     0.9659\n",
            "attention                  39567360     3.9198\n",
            "mlp/ffw1                   39002112     3.8638\n",
            "mlp/ffw2                   39002112     3.8638\n",
            "mlp                        78004224     7.7276\n",
            "block                     117571584    11.6474\n",
            "transformer               470286336    46.5894\n",
            "dense                     539140608    53.4106\n",
            "forward_total            1009426944   100.0000\n",
            "backward_total           2018853888   200.0000\n",
            "total                    3028280832   300.0000\n"
          ]
        }
      ],
      "source": [
        "def flops():\n",
        "    # we only count Weight FLOPs, all other layers (LayerNorm, Softmax, etc) are effectively irrelevant\n",
        "    # we count actual FLOPs, not MACs. Hence 2* all over the place\n",
        "    # basically for any matrix multiply A (BxC) @ B (CxD) -> (BxD) flops are 2*B*C*D\n",
        "\n",
        "    out = OrderedDict()\n",
        "    head_size = n_embd // n_head\n",
        "\n",
        "    # attention blocks\n",
        "    # 1) the projection to key, query, values\n",
        "    out['attention/kqv'] = 2 * block_size * (n_embd * 3*n_embd)\n",
        "    # 2) calculating the attention scores\n",
        "    out['attention/scores'] = 2 * block_size * block_size * n_embd\n",
        "    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "    out['attention/reduce'] = 2 * n_head * (block_size * block_size * head_size)\n",
        "    # 4) the final linear projection\n",
        "    out['attention/proj'] = 2 * block_size * (n_embd * n_embd)\n",
        "    out['attention'] = sum(out['attention/'+k] for k in ['kqv', 'scores', 'reduce', 'proj'])\n",
        "\n",
        "    # MLP blocks\n",
        "    ffw_size = 4*n_embd # feed forward size\n",
        "    out['mlp/ffw1'] = 2 * block_size * (n_embd * ffw_size)\n",
        "    out['mlp/ffw2'] = 2 * block_size * (ffw_size * n_embd)\n",
        "    out['mlp'] = out['mlp/ffw1'] + out['mlp/ffw2']\n",
        "\n",
        "    # the transformer and the rest of it\n",
        "    out['block'] = out['attention'] + out['mlp']\n",
        "    out['transformer'] = n_layer * out['block']\n",
        "    out['dense'] = 2 * block_size * (n_embd * vocab_size)\n",
        "\n",
        "    # forward,backward,total\n",
        "    out['forward_total'] = out['transformer'] + out['dense']\n",
        "    out['backward_total'] = 2 * out['forward_total'] # use common estimate of bwd = 2*fwd\n",
        "    out['total'] = out['forward_total'] + out['backward_total']\n",
        "\n",
        "    return out\n",
        "\n",
        "# compare our param count to that reported by PyTorch\n",
        "f = flops()\n",
        "flops_total = f['forward_total']\n",
        "print(f\"{'name':20s} {'flops':14s} {'ratio (%)':10s}\")\n",
        "for k,v in f.items():\n",
        "    print(f\"{k:20s} {v:14d} {v/flops_total*100:10.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JRbnuEQPhBw",
        "outputId": "52ddf94d-05af-400d-9d85-8b7b4f7830ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "palm_flops: 3028757760, flops: 3028280832, ratio: 1.0002\n"
          ]
        }
      ],
      "source": [
        "# now here is an estimate copy pasted from the PaLM paper\n",
        "# this formula is often used to calculate MFU (model flops utilization)\n",
        "def palm_flops():\n",
        "    \"\"\"estimate of the model flops following PaLM paper formula\"\"\"\n",
        "    # non-embedding model parameters. note that we do not subtract the\n",
        "    # embedding/token params because those are tied and get used in the last layer.\n",
        "    N = params()['total'] - params()['emebedding/position']\n",
        "    L, H, Q, T = n_layer, n_head, n_embd//n_head, block_size\n",
        "    mf_per_token = 6*N + 12*L*H*Q*T\n",
        "    mf = mf_per_token * block_size\n",
        "    return mf\n",
        "\n",
        "print(f\"palm_flops: {palm_flops():d}, flops: {flops()['total']:d}, ratio: {palm_flops()/flops()['total']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q-JhOdlPkJQ",
        "outputId": "ab49545e-4d61-4498-d607-579b61d2dc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fraction of A100 used: 0.66%\n"
          ]
        }
      ],
      "source": [
        "# here is what we currently roughly measure\n",
        "# batch_size = 20 * 5 # 5 is grad_accum, so total batch size is 100\n",
        "measured_time = 0.755 # in seconds per iteration\n",
        "measured_throughput = batch_size / measured_time\n",
        "flops_achieved = f['total'] * measured_throughput\n",
        "\n",
        "# A100 is cited to be 312 TFLOPS of bloat16 running on tensor cores\n",
        "a100_flops_promised = 312e12\n",
        "\n",
        "# the fraction of the A100 that we are using:\n",
        "print(f\"fraction of A100 used: {flops_achieved / a100_flops_promised * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30AIBxp1PpvA"
      },
      "outputs": [],
      "source": [
        "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
        "model_size = params()['total'] # this is number of parameters, N\n",
        "tokens_num = 300e9 # 300B tokens, this is dataset size in tokens, D\n",
        "a100_flops = 312e12 # 312 TFLOPS\n",
        "assumed_mfu = 0.3 # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
        "flops_throughput = a100_flops * 8 * assumed_mfu # assume an 8XA100 node at 30% utilization\n",
        "flops_needed = 6 * model_size * tokens_num # 6ND\n",
        "time_needed_s = flops_needed / flops_throughput # in seconds\n",
        "print(f\"time needed to train the model: {time_needed_s/3600/24:.2f} days\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiczzIhC-h92",
        "outputId": "cd9b63ad-efd1-4379-f815-5772ab3a27c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zBSr4i2zbxR6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mllTCJTm_Hbv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iqZT9ZkEQuFn",
        "OF6FBdyu2ZIu",
        "HFOHDrjMqk4O",
        "bRjzNJdMERLy",
        "_ZfKBotwFC0b",
        "QOuQRSWrFuHD"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}